### Results

---

- The final Training error on the Elmon Network is = 0.001 as compared to one - predictor error of 0.19
- The final training error on the LSTM Network is = 0.0002 as compared to one - predictor error of 0.19
- The final training error on the GRU Network is = 0.0008 as compared to one - predictor error of 0.1944

It is clear that the LSTM and GRU generalize very well, (the best). And it is more computationally expensive than THE standard one. It takes several iterations for the RNN to recognize this pattern of a dot prodcut between the two layers of the input.